{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Implementation of ELM Algorithm on Yeast Dataset\n\nThe dataset selected from the UCI Library link:\n[https://archive.ics.uci.edu/ml/datasets/Yeast](http://)\n\nLibraries used for implementing ELM:\n1. Pandas for data processing and class label visualization\n2. Matplotlib for plotting scatter and line plots\n2. Seaborn for efficient facet visualization\n\nThe dataset can be labelled with headers/ column names in the read.csv command(Parameters) for efficient visualization.\n\nThe dataset has following attributes:\n1. Class Label: 10 Class labels based on other attributes - CYT(0), ERL(1), EXC(2), ME1(3), ME2(4), ME3(5), MIT(6), NUC(7), POX(8) and VAC(9)\n2. mcg: McGeoch's method for signal sequence recognition.\n3. gvh: von Heijne's method for signal sequence recognition.\n4. alm: Score of the ALOM membrane spanning region prediction program.\n5. mit: Score of discriminant analysis of the amino acid content of the N-terminal region (20 residues long) of mitochondrial and non-mitochondrial proteins.\n6. erl: Presence of \"HDEL\" substring (thought to act as a signal for retention in the endoplasmic reticulum lumen). Binary attribute.\n7. pox: Peroxisomal targeting signal in the C-terminus.\n8. vac: Score of discriminant analysis of the amino acid content of vacuolar and extracellular proteins.\n9. nuc: Score of discriminant analysis of nuclear localization signals of nuclear and non-nuclear proteins.\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\nyeast = pd.read_csv(\"../input/yeastwithoutclass/yeast_classrow.csv\", names=['classLabel', 'MCG', 'GVH', 'ALM', 'MIT', 'ERL', 'POX', 'VAC', 'NUC'])\n#csv_dataset.loc[:,['class']].plot()\nprint(\"Database shape rows: %s columns:%s \\n\" % np.shape(yeast))\nprint(yeast.describe())\nyeast.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After fetching the data, it's time to one hot encode the classes of the dataset - Transforming classes into column binary data for convinient array formations. Appending to the function, scatterplots of the class labels have been plotted for getting a rough idea about the count of the class labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_one_hot_encoding(classes, shape):\n    one_hot_encoding = np.zeros(shape)\n    for i in range(0, len(one_hot_encoding)):\n        one_hot_encoding[i][int(classes[i])] = 1\n    return one_hot_encoding\n\n\n#Using Matlplotlib for plotting scatterplots\n\nfig, ax = plt.subplots() \n# count the occurrence of each class \ndata = yeast['classLabel'].value_counts() \nclassLabel = data.index \nfrequency = data.values \n# create bar chart \nax.bar(classLabel, frequency) \n# set title and labels \nax.set_title('Yeast Data labels') \nax.set_xlabel('classLabel') \nax.set_ylabel('Frequency')\n\nyeast.drop(['classLabel'], axis=1).plot.line(title='Yeast Dataset Attributes')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we create a training function considering input(X), output(Y) and hidden layers(H) elements and get the maximum of the parameter. Next we return the Moore-Penrose co-efficeint (pseudo-inverse) of the  matrix(h,0,h) and dot it with 'y'. The Moore-Penrose of the weight matrix is printed in the final output (Output given in the end)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def training(weights, x, y):\n    h = x.dot(weights)\n    h = np.maximum(h, 0, h)\n    pop = np.linalg.pinv(h).dot(y)\n    print(\"The Moore Penrose (Pseudo-Inverse) of the weight matrix is as follows: \")\n    print(pop)\n    return np.linalg.pinv(h).dot(y)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Softmax function, a wonderful activation function that turns numbers aka logits into probabilities that sum to one. \nSoftmax function outputs a vector that represents the probability distributions of a list of potential outcomes\n\n> ***Uncomment the commented lines in the softmax matrix function to see the arrays formed in the final output***"},{"metadata":{"trusted":true},"cell_type":"code","source":"def soft_max(layer):\n    soft_max_output_layer = np.zeros(len(layer))\n    for i in range(0, len(layer)):\n        numitor = 0\n        for j in range(0, len(layer)):\n            numitor += np.exp(layer[j] - np.max(layer))\n        soft_max_output_layer[i] = np.exp(layer[i] - np.max(layer)) / numitor\n    return soft_max_output_layer\n\ndef matrix_soft_max(matrix_):\n    soft_max_matrix = []\n    for i in range(0, len(matrix_)):\n        soft_max_matrix.append(soft_max(matrix_[i]))\n        #popagain = soft_max_matrix\n        #print(popagain)\n    return soft_max_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will check the network's power by initializing the count to Zero and then adding it with the real number to return it."},{"metadata":{"trusted":true},"cell_type":"code","source":"def network_power_check(o, o_real):\n    count = 0\n    for i in range(0, len(o)):\n        count += 1 if np.argmax(o[i]) == np.argmax(o_real[i]) else 0\n    return count","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using Seaborn library to form facetgrids of the class label of Yeast dataset for data columns = 'VAC' & 'NUC'"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\ng = sns.FacetGrid(yeast, col='classLabel')\ng = g.map(sns.kdeplot, 'VAC')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\ng1 = sns.FacetGrid(yeast, col='classLabel')\ng1 = g1.map(sns.kdeplot, 'NUC')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create the testing function with the same parameters as of training, except beta. Here 'Beta'is the variable containing training(weights, x, y) values.\n\n> ***Uncomment the commented line to view the soft max matrix/array in the final output (Before the accuracy line)***"},{"metadata":{"trusted":true},"cell_type":"code","source":"def testing(weights, beta, x, y):\n    h = x.dot(weights)\n    h = np.maximum(h, 0, h)  # ReLU function\n    o = matrix_soft_max(h.dot(beta))\n    #print(o)\n    return network_power_check(o, y) / len(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the end, we will split-shuffle the dataset randomly into training and testing data, keeping the same test size and work the preprocess and normalize operations. All the fucntions will be called at once in the following code and the final output along with the accuracy will be printed. \n\n***The final Accuracy on the Yeast dataset ranges between 50- 65% on each run***"},{"metadata":{"trusted":true},"cell_type":"code","source":"class_column = 0\ntest_size = 0.1\ndb = yeast.iloc[:, :].values.astype(np.float)\nnp.random.shuffle(db)\ny = db[:, class_column]\ny -= np.min(y)\noutput_layer_perceptron_count = len(np.unique(y))\ny = create_one_hot_encoding(y, (len(y), len(np.unique(y))))\nx = np.delete(db, [class_column], axis=1)\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size)\nhidden_layer_perceptron_count = len(y_test)\nx = preprocessing.normalize(x)\nweights = np.random.random((len(x[0]), hidden_layer_perceptron_count))\nbeta = training(weights, x_train, y_train)\nprint(\"The Accuracy of ELM Algorithm on Yeast Dataset is = %s.\" % testing(weights, beta, x_test, y_test))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}